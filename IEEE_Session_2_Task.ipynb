{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mostaf7583/IEEE-tasks/blob/main/IEEE_Session_2_Task.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Welcome to the second session's task!</h1>\n",
        "\n",
        "This task's objectives are to:\n",
        "\n",
        "\n",
        "\n",
        "*   Apply end-to-end machine learning steps on a house prices dataset\n",
        "*   Use a hands-on approach to learning scikit-learn and its uses in machine learning\n",
        "\n"
      ],
      "metadata": {
        "id": "rb8bI-CWfXkJ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bI_6Np90Kyzo"
      },
      "source": [
        "<h1><b>End-to-end Machine Learning</b></h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Steps are to \n",
        "\n",
        "\n",
        "\n",
        "1.   Download the data\n",
        "2.   Understand and Clean the Data\n",
        "3.   Preprocess and Feature Engineer the Data\n",
        "4.   Train a model using the data\n",
        "5.   Measure the model's performance using cross validation\n",
        "6.   Do hyperparameter tuning on the model and/or try different models\n",
        "7.   Use the final selected model to make predictions on the test set\n",
        "\n"
      ],
      "metadata": {
        "id": "iSp7BwiZw9zS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIOyuYMkKyz-"
      },
      "source": [
        "## Download the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This part really depends on the problem and how it's presented to you. \n",
        "\n",
        "For example, I have uploaded the data to Kaggle so you can download it from there. \n",
        "\n",
        "Google Colab also provides the option to upload files but those will be deleted once the Colab session ends so it's better to download the data using code so you can re-run that cell every time you want to download the data.\n",
        "\n",
        "For this task, however, I have uploaded the data to Google Drive and it can be downloaded as so:"
      ],
      "metadata": {
        "id": "CF9mjNUPrL-F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Extra information in case you're wondering what the next cell does:\n",
        "</h2>\n",
        "\n",
        "\n",
        "The exclamation mark allows us to use bash commands, which is a way to use the operating system from the command line.\n",
        "\n",
        "Most common and useful commands are:\n",
        "\n",
        "1. pwd \n",
        "\n",
        "(short for print working directory to print where we are executing the command)\n",
        "\n",
        "2. ls \n",
        "\n",
        "(short for list directory to list directory contents)\n",
        "\n",
        "3. cd \n",
        "\n",
        "(short for change directory to change our current directory)\n",
        "\n",
        "\n",
        "4. mkdir \n",
        "\n",
        "(short for make directory to create a new directory)\n",
        "\n",
        "5. mv \n",
        "\n",
        "(short for move to move a file or folder from one location to another)\n",
        "\n",
        "6. cp \n",
        "\n",
        "(short for copy where it copies a file from one location to another)\n",
        "\n",
        "\n",
        "So gdown is actually a program that comes pre-installed with colab and it allows us to download a file from Google Drive using its id."
      ],
      "metadata": {
        "id": "vEpzxdDEsgUy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1lXJTBP4mOVnS2nMTusEcUHjqWgZGNkLW\n",
        "!gdown 1U2IWwN78Td1XQz38_u60pXsU_WgQzCtB\n",
        "!gdown 1RwXmgcY6rh-JPgGHWLRn5lzom8YMIIFC"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wx4eAu6EsEni",
        "outputId": "55e107d4-22e9-42ab-eb98-cf70e37048a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1lXJTBP4mOVnS2nMTusEcUHjqWgZGNkLW\n",
            "To: /content/house_prices_train.csv\n",
            "100% 1.06M/1.06M [00:00<00:00, 24.8MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1U2IWwN78Td1XQz38_u60pXsU_WgQzCtB\n",
            "To: /content/house_prices_test.csv\n",
            "100% 208k/208k [00:00<00:00, 19.2MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1RwXmgcY6rh-JPgGHWLRn5lzom8YMIIFC\n",
            "To: /content/sample_submission.csv\n",
            "100% 49.8k/49.8k [00:00<00:00, 35.4MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epZTgIf8Kyz9"
      },
      "source": [
        "<h2>Our goal for this task is to use the house_prices_train.csv dataset to create a model and use that model to predict prices for house_prices_test.csv\n",
        "\n",
        "We will then submit the given price predictions to Kaggle in the same format as the file sample_submission.csv</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Jni2de1Kyz_"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "#Helps make printing numbers look better\n",
        "pd.options.display.float_format = '{:.2f}'.format\n",
        "\n",
        "train = pd.read_csv('house_prices_train.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dR801oUKy0A"
      },
      "source": [
        "## Take a Quick Look at the Data Structure"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1><b>Q1: Use some pandas built-in functions like head, describe, value_counts, etc. to take an initial look at the data</b></h1>"
      ],
      "metadata": {
        "id": "X7jDgpnm36MU"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7SsheSSMPe5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6ldkF-QKy0V"
      },
      "source": [
        "# Discover and Visualize the Data to Gain Insights"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1><b>Q2: Use any visualization library you like to visualize some of the datasets attributes. Hint: You can use pandas dataframes built-in hist and box plot methods or pandas scatter_matrix. Other solutions are also welcome.</b></h1>"
      ],
      "metadata": {
        "id": "QGR31vYQ4TOn"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EHsqCFm8PgS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fq6_q9opKy0X"
      },
      "source": [
        "## Looking for Correlations"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1><b>Q3: Look for correlations using pandas df.corr function, this will give you an output correlation matrix. Select the price column from that correlation matrix and sort its values descendingly  using a built-in pandas function.</b></h1>"
      ],
      "metadata": {
        "id": "BUFV6Hqv4tYk"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q_g8CRM7PhfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9P0S9d4GKy0Y"
      },
      "source": [
        "# Prepare the Data for Machine Learning Algorithms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YyAYK_nTKy0Z"
      },
      "source": [
        "## Data Cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1><b>Q4: Drop the ad_id column from the training set.</b></h1>"
      ],
      "metadata": {
        "id": "PBZViPD9GaPE"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jeRMdfuzGd4y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1><b>Q5: Check how many NaN values are there in the dataset. Demonstrate the three ways we've covered to deal with NaN values</b></h1>"
      ],
      "metadata": {
        "id": "mYRQw-5Y6q3_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0ccnkQtKPkvl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1><b>Q5.1: Demonstrate how to drop the furnished column entirely (don't alter the train dataframe)</b></h1>"
      ],
      "metadata": {
        "id": "Hni_KYI37OD_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kySYM36ZPmrh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1><b>Q5.2: Demonstrate how to drop the rows which have a furnished value of NaN (don't alter the train dataframe)</b></h1>"
      ],
      "metadata": {
        "id": "xtsodn6C7X6C"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sI-HzCBLPqHZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1><b>Q5.3: Demonstrate how to count the most frequent value in the furnished column and how to fill na values with this value using pandas fillna function (don't alter the train dataframe)</b></h1>"
      ],
      "metadata": {
        "id": "_BSabd047em5"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fXsC8JcIPrfv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next cell is to illustrate how you could solve Q5.3 using sklearn's SimpleImputer. You can use this as a reference for Q7."
      ],
      "metadata": {
        "id": "BJLbKYsG9HC1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CqLwYvHVKy0a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b168f56-a308-451c-df2c-7e2a3946cf8e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([['no'],\n",
              "       ['no'],\n",
              "       ['no'],\n",
              "       ...,\n",
              "       ['no'],\n",
              "       ['no'],\n",
              "       ['no']], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "columns_to_impute = ['furnished']\n",
        "\n",
        "imputer = SimpleImputer(strategy=\"most_frequent\")\n",
        "\n",
        "imputer.fit_transform(train[columns_to_impute])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Engineering"
      ],
      "metadata": {
        "id": "GFICInABRhDO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1><b>Q6: You're free to experiment here with adding any features that could help improve the model's performance. Make sure to add them within the feature_engineer function</b></h1>"
      ],
      "metadata": {
        "id": "z0dWWSkU5P8j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def feature_engineer(df):\n",
        "  new_df = df.copy()\n",
        "  new_df['area_to_bathroom_ratio'] = new_df.area / new_df.bathrooms\n",
        "  return new_df\n",
        "\n",
        "train = feature_engineer(train)"
      ],
      "metadata": {
        "id": "CVPYlSPoeBO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "65FeYsKURoqt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1><b>Q6: Demonstrate how to use sklearn's OneHotEncoder on the categorical columns. Use the cat_columns and num_columns lists to quickly specify categorical or numerical attributes.</b></h1>"
      ],
      "metadata": {
        "id": "lNtuVDsA8SLC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Don't forget to add any column names you created during feature engineering here\n",
        "cat_columns = ['type', 'bedrooms', 'bathrooms', 'level', 'furnished', 'rent', 'city', 'region']\n",
        "\n",
        "num_columns = ['area', 'area_to_bathroom_ratio']"
      ],
      "metadata": {
        "id": "f-imW-Dr22P9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ksPMaTz1Ky0g"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "cat_encoder = # Call the OneHotEncoder constructor here\n",
        "train_cat_one_hot = #Call the cat_encoder's fit_transform function on the train set's cat columns here"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the previous cell was correct, you should see the one hot encoded outputs as a DataFrame by runnning the next cell"
      ],
      "metadata": {
        "id": "BL5tvG-KTxOg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cat_one_hot_df = pd.DataFrame(train_cat_one_hot.toarray(), \n",
        "                              columns= cat_encoder.get_feature_names_out(), \n",
        "                              index = train.index)\n",
        "cat_one_hot_df"
      ],
      "metadata": {
        "id": "zWpuXvOnUAYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agQ8BJIWKy0j"
      },
      "source": [
        "<h1><b>Q7: Demonstrate how to use sklearn's MinMaxScaler on the numerical columns. Use the cat_columns and num_columns lists to quickly specify categorical or numerical attributes. Use the same process as OneHotEncoder but for num_columns</b></h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4FC9feNEKy0k"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "All previous preprocessing steps were for demonstration purposes. We'll actually be using sklearn's pipeline feature to construct our preprocessing pipeline like so:"
      ],
      "metadata": {
        "id": "vHjcbi0WUONY"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVr_4ajeKy0r"
      },
      "source": [
        "## Transformation Pipelines"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next cell is an example for how to create a pipeline for categorical features only. It's a way to execute all the preprocessing steps using one call to cat_pipeline"
      ],
      "metadata": {
        "id": "2BBi1d_4UdSn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "cat_pipeline = make_pipeline(\n",
        "    SimpleImputer(strategy=\"most_frequent\"),\n",
        "    OneHotEncoder(handle_unknown=\"ignore\"))\n",
        "\n",
        "cat_pipeline.fit_transform(train[cat_columns])"
      ],
      "metadata": {
        "id": "iAd2I0v_VKXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lwiM6C-bKy0s",
        "outputId": "0233ca73-6ba8-4a76-d688-1be248990916",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('impute', SimpleImputer(strategy='median')),\n",
              " ('min_max_scaling', MinMaxScaler())]"
            ]
          },
          "metadata": {},
          "execution_count": 157
        }
      ],
      "source": [
        "cat_pipeline.steps"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1><b>Q8: Define num_pipeline in the same way as cat_pipeline above but using SimpleImputer(strategy=\"median\") and MinMaxScaler() instead</b></h1>"
      ],
      "metadata": {
        "id": "xHwRthAJVPKx"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eKjr2Y8OVzJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next cell shows how to combine the two pipelines for numerical and categorical features to create our preprocessing pipeline for all features"
      ],
      "metadata": {
        "id": "3qjT_3cWVzkY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2JD_0oqEKy0u"
      },
      "outputs": [],
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "preprocessing = ColumnTransformer([\n",
        "    (\"num\", num_pipeline, num_columns),\n",
        "    (\"cat\", cat_pipeline, cat_columns),\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1><b>Q9: Define a variable X equal to all columns in the train dataset except price. Define another variable y that's just the price column</b></h1>"
      ],
      "metadata": {
        "id": "hhZJCOnyWAR5"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QlPapa8Jwhdz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUq8KLVIKy0v"
      },
      "source": [
        "# Select and Train a Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sR9KdRErKy0w"
      },
      "source": [
        "## Training and Evaluating on the Training Set"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following cell demonstrates how to import a simple LinearRegression model and that you can add models at the end of existing pipelines to make a pipeline for preprocessing and training"
      ],
      "metadata": {
        "id": "m8fnFcQcWNhZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k-HxuwKwKy0w",
        "outputId": "08098d02-44d0-4d3a-8d79-94e24fa13901",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(steps=[('columntransformer',\n",
              "                 ColumnTransformer(transformers=[('num',\n",
              "                                                  Pipeline(steps=[('impute',\n",
              "                                                                   SimpleImputer(strategy='median')),\n",
              "                                                                  ('min_max_scaling',\n",
              "                                                                   MinMaxScaler())]),\n",
              "                                                  ['area']),\n",
              "                                                 ('cat',\n",
              "                                                  Pipeline(steps=[('simpleimputer',\n",
              "                                                                   SimpleImputer(strategy='most_frequent')),\n",
              "                                                                  ('onehotencoder',\n",
              "                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n",
              "                                                  ['type', 'bedrooms',\n",
              "                                                   'bathrooms', 'level',\n",
              "                                                   'furnished', 'rent', 'city',\n",
              "                                                   'region'])])),\n",
              "                ('linearregression', LinearRegression())])"
            ]
          },
          "metadata": {},
          "execution_count": 160
        }
      ],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "lin_reg = make_pipeline(preprocessing, LinearRegression())\n",
        "#This fits the model with the X as input and y as the target\n",
        "lin_reg.fit(X, y) \n",
        "#This is making predictions on the training set which doesn't make sense\n",
        "#It's only used here to demonstrate how to use the model to make predictions\n",
        "predictions = lin_reg.predict(X) \n",
        "#Let's see the RMSE for the model on the training set\n",
        "#Look at the documentation for mean_squared_error for more info\n",
        "lin_rmse = mean_squared_error(y, predictions,\n",
        "                              squared=False)\n",
        "lin_rmse"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1><b>Q10: Import the RandomForestRegressor model and create a pipeline like the previous cell but with RandomForestRegressor instead of LinearRegressor. Don't forget to set the random_state parameter to any number to get reproducible results</b></h1>"
      ],
      "metadata": {
        "id": "z98EGIazY5bO"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rKyJS4D4ak2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geYRFEMHKy0y"
      },
      "source": [
        "## Better Evaluation Using Cross-Validation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1><b>Q11: Now import and use the cross_val_score function from sklearn. You need to pass in your model's pipeline, the X and y, and then specify the scoring metric to be \"neg_root_mean_squared_error\". </b></h1>"
      ],
      "metadata": {
        "id": "ZwkESkMGaJEK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: This might take a few minutes to run!"
      ],
      "metadata": {
        "id": "NFfBowVhasF5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another Note: The cross_val_score function already splits the training data into training and validation data and reports the validation score.\n",
        "There's no test data for you to split because I had already split it and won't make it available to you till later on :P"
      ],
      "metadata": {
        "id": "miTbIMYRbYHY"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gxxIuQILZt8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jqVCScJKy0z"
      },
      "source": [
        "# Fine-Tune Your Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYlMTc7JKy00"
      },
      "source": [
        "## Grid Search"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following demonstrates how to use grid search to do exhaustive search using the full pipeline (preprocessing + model).\n",
        "\n",
        "It tries all combinations of the provided values and selects the model with the best score."
      ],
      "metadata": {
        "id": "qfupzsFMbTih"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-J20SYlVKy00"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "full_pipeline = Pipeline([\n",
        "    (\"preprocessing\", preprocessing),\n",
        "    (\"random_forest\", RandomForestRegressor(random_state=42)),\n",
        "])\n",
        "param_grid = [\n",
        "    {'random_forest__n_estimators': [16, 32],\n",
        "     'random_forest__max_features': [4, 6, 8]},\n",
        "    {'random_forest__n_estimators': [64, 128],\n",
        "     'random_forest__max_features': [6, 8, 10]},\n",
        "]\n",
        "grid_search = GridSearchCV(full_pipeline, param_grid, cv=3,\n",
        "                           scoring='neg_root_mean_squared_error')\n",
        "grid_search.fit(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8AVveWebKy00"
      },
      "source": [
        "The best hyperparameter combination found:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-2MeKJq-Ky00"
      },
      "outputs": [],
      "source": [
        "grid_search.best_params_"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model with the best score will be available as the best_estimator_ attribute of the grid_search object.\n",
        "\n",
        "Since we are using pipelines, the final model is actually a pipeline which means it applies the preprocessing pipeline steps to its input and then the model prediction step"
      ],
      "metadata": {
        "id": "avZfbVhOcBUo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZZ-Uz-N5Ky01"
      },
      "outputs": [],
      "source": [
        "final_model = grid_search.best_estimator_"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_model"
      ],
      "metadata": {
        "id": "AlEpv0Ztcdrg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqXx7W6OKy01"
      },
      "source": [
        "Let's look at the score of each hyperparameter combination tested during the grid search:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CXDvtEQbKy01"
      },
      "outputs": [],
      "source": [
        "cv_res = pd.DataFrame(grid_search.cv_results_)\n",
        "cv_res.sort_values(by=\"mean_test_score\", ascending=False, inplace=True)\n",
        "\n",
        "cv_res.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npb4_cldKy04"
      },
      "source": [
        "## Evaluate Your System on the Test Set"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we'll load the test set, run our feature engineering function on it, and then pass it to our model's predict function (which does the preprocessing steps before predicting because it's a pipeline object)"
      ],
      "metadata": {
        "id": "GRpigYjaaBp6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test = pd.read_csv('house_prices_test.csv')\n",
        "test = feature_engineer(test)\n",
        "test_predictions = final_model.predict(test)"
      ],
      "metadata": {
        "id": "EOIcSt0_JTl7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have a sample submission file demonstrating the format which we should submit our predictions. We'll just replace the price column with our predictions here and save the file"
      ],
      "metadata": {
        "id": "pPyz_03rcrjz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "submission = pd.read_csv('sample_submission.csv')\n",
        "submission.price = test_predictions\n",
        "\n",
        "submission"
      ],
      "metadata": {
        "id": "9eQn__LZjeLJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the file you should submit to kaggle. You can download it from the menu on the left of the colab website. "
      ],
      "metadata": {
        "id": "sJ4519ryc8e-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "submission.to_csv('submission.csv', index=False)"
      ],
      "metadata": {
        "id": "x-Qlel71YYM8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SvVg3MvVKy1E"
      },
      "source": [
        "<h1><b>You made it till the end. Awesome. 🔥🔥🔥</b></h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You're of course free to continue tinkering with the task after finishing it to try and get a better score (which will be reflected on the Kaggle leaderboard!)\n",
        "\n",
        "You can try playing with different features in the feature engineering step, or importing and trying different models and/or hyperparameters, or maybe different preprocessing steps.\n",
        "\n",
        "The possibilities are endless!\n"
      ],
      "metadata": {
        "id": "Ff-9n00Kdb5y"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "nav_menu": {
      "height": "279px",
      "width": "309px"
    },
    "toc": {
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": "block",
      "toc_window_display": false
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}